import {
  Configuration,
  CreateCompletionRequest,
  OpenAIApi,
  CreateChatCompletionRequest,
} from 'openai';
import { completionParams, userOptions } from '../constant';
import { generatePrompt } from '../prompt';
import { HuskyGPTTypeEnum, IReadFileResult } from '../types';

/**
 * OpenAI Factory
 * Usage:
 * const openai = new OpenAIFactory();
 * const result = await openai.run({ filePath });
 */
class OpenAIFactory {
  private configuration: Configuration;
  private openai: OpenAIApi;

  constructor() {
    // Create a new OpenAI API client configuration
    this.configuration = new Configuration({
      apiKey: userOptions.openAIKey,
    });

    // Create a new OpenAI API client
    this.openai = new OpenAIApi(this.configuration);
  }

  private openAICompletionMap: Record<
    HuskyGPTTypeEnum,
    (prompt: string) => Promise<string>
  > = {
    [HuskyGPTTypeEnum.Test]: this.openAICreateCompletion.bind(this),
    [HuskyGPTTypeEnum.Review]: this.openAIChatCompletion.bind(this),
  };

  private get completionParams(): CreateCompletionRequest {
    const options: CreateCompletionRequest = {
      ...completionParams,
      ...userOptions.openAIOptions,
    };

    return options;
  }

  private get chatCompletionParams(): CreateChatCompletionRequest {
    const completionParams = this.completionParams;
    const options: CreateChatCompletionRequest = {
      model: completionParams.model,
      messages: [],
      temperature: completionParams.temperature,
      max_tokens: completionParams.max_tokens!,
      top_p: completionParams.top_p,
      stop: completionParams.stop as string[],
      frequency_penalty: completionParams.frequency_penalty,
      presence_penalty: completionParams.presence_penalty,
    };

    return options;
  }

  /**
   * Generate prompt for the OpenAI API
   */
  private generatePrompt(fileResult: IReadFileResult): string {
    // Set the file content as the prompt for the API request
    const prompt = `
      ${generatePrompt(fileResult)}
      ###
    `;

    if (process.env.DEBUG) {
      console.log('prompt ===>', prompt);
    }

    return prompt;
  }

  /**
   * Generate a test message using the OpenAI API
   */
  private async openAICreateCompletion(prompt: string): Promise<string> {
    // Create a new chat completion, using the GPT-3.5 Turbo model
    const completion = await this.openai.createCompletion({
      ...this.completionParams,
      prompt,
    });

    // Print the message generated by the API
    const result = completion.data.choices[0].text;

    if (process.env.DEBUG) {
      console.log('createCompletion usage ===>', completion.data.usage);
    }

    return result || '';
  }

  /**
   * Generate a review message using the OpenAI API chat completion
   */
  private async openAIChatCompletion(prompt: string): Promise<string> {
    // Create a new chat completion, using the GPT-3.5 Turbo model
    const completion = await this.openai.createChatCompletion({
      ...this.chatCompletionParams,
      messages: [{ role: 'user', content: prompt }],
    });

    if (process.env.DEBUG) {
      console.log('createChatCompletion usage ===>', completion.data.usage);
    }

    // Print the message generated by the API
    const result = completion.data.choices[0].message?.content;

    return result || '';
  }

  /**
   * Run the OpenAI API
   * @description filePath is the path of the file to be passed to the OpenAI API as the prompt
   * @returns {Promise<string>}
   */
  async run(fileResult: IReadFileResult): Promise<string> {
    const prompt = this.generatePrompt(fileResult);
    const message = await this.openAICompletionMap[userOptions.huskyGPTType](
      prompt
    );

    return message;
  }
}

export default OpenAIFactory;
